{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Run the code cells below in order to get the final results**\n",
    "1. First cell defines functions.\n",
    "2. Second cell produces the fidelity and utility results of **news apps**.\n",
    "3. Third cell produces the fidelity and utility results of **shopping apps**.\n",
    "4. Last cell processes the results and adds other handy information. \n",
    "It outputs the final results of all the test reuse cases, including `shopping_final.csv` for the 10 shopping apps and `news_final.csv` for the 10 news apps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import json\n",
    "import copy\n",
    "import math\n",
    "from pandas.io.parsers import read_csv\n",
    "from lxml import etree\n",
    "\n",
    "def evaluate_accuracy(test):\n",
    "    ground_truth_source = read_csv(\"gui_mapper/ground_truth_mapping/\" + app_category + \"/GT_\" + test['source'] + \".csv\")\n",
    "    ground_truth_target = read_csv(\"gui_mapper/ground_truth_mapping/\" + app_category + \"/GT_\" + test['target'] + \".csv\")\n",
    "    \n",
    "    source_test = ground_truth_tests.loc[ground_truth_tests['method'] == test['method']]\n",
    "    if source_test.shape[0] != 1:\n",
    "        print(source_test, 'is not 1')\n",
    "    \n",
    "    for gui_event, source_gui_event in zip(test['event_array'], source_test.iloc[0]['event_array']):\n",
    "        if source_gui_event['id_or_xpath'][:3] == \"id@\":\n",
    "            source_event = ground_truth_source.loc[ground_truth_source['id'] == source_gui_event['id_or_xpath'][3:]]\n",
    "        else:\n",
    "            source_event = ground_truth_source.loc[ground_truth_source['xpath'] == source_gui_event['id_or_xpath'][6:]]\n",
    "        if source_event.shape[0] == 0:\n",
    "            print('L84 gt missing for', source_gui_event, 'check if it should be added')\n",
    "        if pd.isnull(gui_event['id_or_xpath']) or gui_event['id_or_xpath'] == '': \n",
    "            # this is a sanity check\n",
    "            # if not transferred to any event, it should be marked as NONE\n",
    "            print('missed is not marked as NONE', gui_event)\n",
    "        if gui_event['id_or_xpath'] != \"NONE\": # check correct or incorrect\n",
    "            if gui_event['id_or_xpath'][:3] == \"id@\":\n",
    "                transfer_event = ground_truth_target.loc[ground_truth_target['id'] == gui_event['id_or_xpath'][3:]]\n",
    "            else:\n",
    "                transfer_event = ground_truth_target.loc[ground_truth_target['xpath'] == gui_event['id_or_xpath'][6:]]\n",
    "            if transfer_event.shape[0] == 0:\n",
    "                # print('L95 gt missing for', gui_event, 'check if it should be added')\n",
    "                gui_event['case'] = \"incorrect\"\n",
    "                # print(\"transfer_event\", test['source'], test['target'], test['gui_mapper'])\n",
    "                # print(test['method'])\n",
    "                # print(gui_event['id_or_xpath'])\n",
    "            elif transfer_event.iloc[0]['canonical'] == source_event.iloc[0]['canonical']:\n",
    "                gui_event['case'] = \"correct\"\n",
    "            else:\n",
    "                gui_event['case'] = \"incorrect\"\n",
    "\n",
    "        else: # check miss or nonExist\n",
    "            target_event = ground_truth_target.loc[ground_truth_target['canonical'] == source_event.iloc[0]['canonical']]\n",
    "            if target_event.shape[0] != 0:\n",
    "                gui_event['case'] = \"missed\"\n",
    "            else:\n",
    "                gui_event['case'] = \"nonExist\"\n",
    "                \n",
    "    return test\n",
    "\n",
    "\n",
    "def list_cases(test):\n",
    "    cases = {'correct' : [], 'incorrect' : [], 'missed' : [], 'nonExist' : []}\n",
    "    for gui_event in test:\n",
    "        cases[gui_event['case']].append(gui_event['id_or_xpath'])\n",
    "    return cases\n",
    "\n",
    "def count_cases(test):\n",
    "    cases = {}\n",
    "    cases['num_correct'] = len(test['correct'])\n",
    "    cases['num_incorrect'] = len(test['incorrect'])\n",
    "    cases['num_missed'] = len(test['missed'])\n",
    "    cases['num_nonExist'] = len(test['nonExist'])\n",
    "    # cases['num_TP'] = len(test['TP'])\n",
    "    # cases['num_FP'] = len(test['FP'])\n",
    "    # cases['num_FN'] = len(test['FN'])\n",
    "    return cases\n",
    "\n",
    "def calc_precision_recall_accuracy(test):\n",
    "    fractions = {}\n",
    "    try:\n",
    "        fractions['accuracy_precision'] = test['num_correct'] / (test['num_correct'] + test['num_incorrect'])\n",
    "    except ZeroDivisionError:\n",
    "        fractions['accuracy_precision'] = np.NaN\n",
    "    try:\n",
    "        fractions['accuracy_recall'] = test['num_correct'] / (test['num_correct'] + test['num_missed'])\n",
    "    except ZeroDivisionError:\n",
    "        fractions['accuracy_recall'] = np.NaN\n",
    "    try:\n",
    "        fractions['accuracy'] = (test['num_correct'] + test['num_nonExist']) / \\\n",
    "                                (test['num_correct'] + test['num_incorrect'] + test['num_missed'] + test['num_nonExist'])\n",
    "    except ZeroDivisionError:\n",
    "        fractions['accuracy'] = np.NaN\n",
    "    # try:\n",
    "    #     fractions['effectiveness_precision'] = test['num_TP'] / (test['num_TP'] + test['num_FP'])\n",
    "    # except ZeroDivisionError:\n",
    "    #     fractions['effectiveness_precision'] = np.NaN\n",
    "    # try:\n",
    "    #     fractions['effectiveness_recall'] = test['num_TP'] / (test['num_TP'] + test['num_FN'])\n",
    "    # except ZeroDivisionError:\n",
    "    #     fractions['effectiveness_recall'] = np.NaN\n",
    "    return fractions\n",
    "\n",
    "def append_src_gt_events(test):\n",
    "    events = {}\n",
    "    # add src events\n",
    "    src_events = ground_truth_tests.loc[ground_truth_tests['method'] == test['method']]\n",
    "    if src_events.shape[0] == 1:\n",
    "        events['src_events'] = [gui_event['id_or_xpath'] for gui_event in src_events.iloc[0]['event_array']]\n",
    "    else:\n",
    "        print('src events len is not 1, check: ', test['method'])\n",
    "    # add gt events\n",
    "    if app_category == 'shopping':\n",
    "        source_app = app_name_mapping[test['source']]\n",
    "        target_app = app_name_mapping[test['target']]\n",
    "    if app_category == 'news':\n",
    "        source_app = test['source']\n",
    "        target_app = test['target']\n",
    "    target_method = test['method'].replace(source_app, target_app)\n",
    "    gt_test = ground_truth_tests.loc[ground_truth_tests['method'] == target_method]\n",
    "    if gt_test.shape[0] == 1:\n",
    "        events['gt_events'] = [gui_event['id_or_xpath'] for gui_event in gt_test.iloc[0]['event_array']]\n",
    "    else:\n",
    "        if gt_test.shape[0] > 1:\n",
    "            print('gt events len > 1, check: ', target_method)    \n",
    "            # print(gt_test.shape)\n",
    "            # print(gt_test)\n",
    "    return events\n",
    "\n",
    "def get_classname_from_xpath(xpath):\n",
    "    return xpath.split('//')[1].split('[')[0]\n",
    "\n",
    "def get_attribute_from_xpath(xpath):\n",
    "    if '\"\"' in xpath:\n",
    "        xpath = xpath.replace('\"\"', '\"')\n",
    "    return xpath[xpath.find(\"[\") + 1:xpath.find(\"]\")]\n",
    "\n",
    "def find_node_by_xpath(xpath, app):\n",
    "    directory = 'input/screenshots/' + app_category + '/' + app + '/'\n",
    "    # print('find node for xpath', xpath, 'in app', app)\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".uix\"):\n",
    "            # print('check xpath in ', os.path.join(directory, filename))\n",
    "            tree = etree.parse(os.path.join(directory, filename))\n",
    "            root = tree.getroot()\n",
    "            if xpath.startswith('//'):  # relative xpath\n",
    "                class_name = get_classname_from_xpath(xpath)\n",
    "                attribute = get_attribute_from_xpath(xpath)\n",
    "                # print('//node[@class=\"'+class_name+'\"]['+attribute+']')\n",
    "                nodes = root.xpath('//node[@class=\"' + class_name + '\"][' + attribute + ']')\n",
    "                if len(nodes) != 0:\n",
    "                    # print('current node is ', etree.tostring(nodes[0]))\n",
    "                    return nodes[0]\n",
    "            elif xpath.startswith('/hierarchy'):  # absolute xpath\n",
    "                class_names = xpath.split('/')\n",
    "                # print(class_names)\n",
    "                current_node = root.xpath('/hierarchy')[0]\n",
    "                no_matching = False\n",
    "                for class_name in class_names:\n",
    "                    if class_name == '' or class_name == 'hierarchy':\n",
    "                        continue\n",
    "                    # print('.//node[@class=\"' + class_name +'\"]')\n",
    "                    if '[' in class_name:  # multiple children with same class name\n",
    "                        index = int(class_name[class_name.find(\"[\") + 1:class_name.find(\"]\")])\n",
    "                        class_name = class_name.split('[')[0]\n",
    "                        current_nodes = current_node.findall('./node[@class=\"' + class_name + '\"]')\n",
    "                        if current_nodes is None or index >= len(current_nodes):\n",
    "                            no_matching = True\n",
    "                            break\n",
    "                        else:\n",
    "                            current_node = current_nodes[index]\n",
    "                    else:  # only one child with same class name\n",
    "                        current_nodes = current_node.findall('./node[@class=\"' + class_name + '\"]')\n",
    "                        if current_nodes is None or len(current_nodes) == 0:\n",
    "                            no_matching = True\n",
    "                            break\n",
    "                        else:\n",
    "                            current_node = current_nodes[0]\n",
    "                if not no_matching:\n",
    "                    # print('current node is ', etree.tostring(current_node))\n",
    "                    return current_node\n",
    "    # print('current node is None')\n",
    "    return None\n",
    "\n",
    "# trans test format: json with \"input\", \"id_or_xpath\", \"action\", \"case\". 'id_or_xpath' could be 'NONE'\n",
    "# gt test format: 'id@...'/'xpath@...'\n",
    "def trans_equals_gt(trans_event, gt_event, tgt_app):\n",
    "    # when trans and gt use the same id or xpath\n",
    "    trans_id_or_xpath = trans_event['id_or_xpath'] \n",
    "    if gt_event == trans_id_or_xpath:\n",
    "        return True\n",
    "    if gt_event[:3] == \"id@\": # gt_event is based on resource-id\n",
    "        if trans_id_or_xpath[:3] == \"id@\":\n",
    "            return False\n",
    "        else: # gt uses id and trans uses xpath\n",
    "            return compare_id_xpath(gt_event[3:], trans_id_or_xpath[6:], tgt_app)\n",
    "    else: # gt_event is based on xpath\n",
    "        if trans_id_or_xpath[:3] == \"id@\": # trans uses id, gt uses xpath\n",
    "            return compare_id_xpath(trans_id_or_xpath[3:], gt_event[6:], tgt_app)\n",
    "        else: # both gt and trans use xpath. one could use absolute xpath and another one uses relevant xpath\n",
    "            gt_node = find_node_by_xpath(gt_event[6:], tgt_app)\n",
    "            trans_node = find_node_by_xpath(trans_id_or_xpath[6:], tgt_app)\n",
    "            return (gt_node == trans_node)\n",
    "\n",
    "def compare_id_xpath(id, xpath, app):\n",
    "    node = find_node_by_xpath(xpath, app) \n",
    "    if node is not None and id == node.get('resource-id'):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# trans test format: json with \"input\", \"id_or_xpath\", \"action\", \"case\". 'id_or_xpath' could be 'NONE'\n",
    "# gt test format: 'id@...'/'xpath@...'\n",
    "# return the levenshtein distance\n",
    "def levenshtein(test):\n",
    "    transfer_seq = test['event_array']\n",
    "    gt_seq = test['gt_events']\n",
    "    result = {}\n",
    "    # if gt test doesn't exist in the target app, return NA\n",
    "    if type(gt_seq) is float and math.isnan(gt_seq):\n",
    "        # print(gt_seq)\n",
    "        result['distance'] = np.NaN\n",
    "        return result\n",
    "    \n",
    "    trans = copy.deepcopy(transfer_seq)\n",
    "    trans = json.loads(trans)\n",
    "    gt = copy.deepcopy(gt_seq)\n",
    "    \n",
    "    # delete 'NONE' events in order to calculate levenshtein distance correctly\n",
    "    # print('before trans = ', trans)\n",
    "    none_events = []\n",
    "    for event in trans:\n",
    "        if event['id_or_xpath'] == 'NONE':\n",
    "            none_events.append(event)\n",
    "    for event in none_events:\n",
    "        trans.remove(event)\n",
    "    # print('trans = ', trans)\n",
    "    # print('gt = ', gt)\n",
    "\n",
    "    size_x = len(trans) + 1\n",
    "    size_y = len(gt) + 1\n",
    "    matrix = np.zeros ((size_x, size_y))\n",
    "    for x in range(size_x):\n",
    "        matrix [x, 0] = x\n",
    "    for y in range(size_y):\n",
    "        matrix [0, y] = y\n",
    "\n",
    "    for x in range(1, size_x):\n",
    "        for y in range(1, size_y):\n",
    "            if trans_equals_gt(trans[x-1], gt[y-1], test['target']):\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1, y] + 1,\n",
    "                    matrix[x-1, y-1],\n",
    "                    matrix[x, y-1] + 1\n",
    "                )\n",
    "            else:\n",
    "                matrix [x,y] = min(\n",
    "                    matrix[x-1,y] + 1,\n",
    "                    matrix[x-1,y-1] + 1,\n",
    "                    matrix[x,y-1] + 1\n",
    "                )\n",
    "    # print (matrix)\n",
    "    # print('distance = ', (matrix[size_x - 1, size_y - 1]))\n",
    "    result['distance'] =  (matrix[size_x - 1, size_y - 1])\n",
    "    return result\n",
    "\n",
    "def count_event_num_other(test):\n",
    "    num_events = {}\n",
    "    num_events['num_src'] = len(eval(test['src_events']))\n",
    "    transferred_json = json.loads(test['transferred'])\n",
    "    count = 0\n",
    "    for trans in transferred_json:\n",
    "        if trans['id_or_xpath'] != 'NONE':\n",
    "            count += 1\n",
    "    num_events['num_trans'] = count\n",
    "    if pd.isnull(test['gt_events']):\n",
    "        num_events['num_gt'] = np.NaN\n",
    "    else:\n",
    "        num_events['num_gt'] = len(eval(test['gt_events']))\n",
    "    return num_events\n",
    "\n",
    "def count_event_num_atm(test):\n",
    "    num_events = {}\n",
    "    num_events['num_src'] = len(eval(test['src_events']))\n",
    "    transferred_json = json.loads(test['transferred'])\n",
    "    count = 0\n",
    "    for trans in transferred_json:\n",
    "        if trans != {}:\n",
    "            count += 1\n",
    "    num_events['num_trans'] = count\n",
    "    num_events['num_gt'] = len(eval(test['gt_events']))\n",
    "    return num_events\n",
    "\n",
    "def calculate_utility_atm(test):\n",
    "    result = {}\n",
    "    if np.isnan(test['num_gt']):\n",
    "        result['reduction'] = np.NaN\n",
    "        return result\n",
    "    try:\n",
    "        result['reduction'] = (test['num_gt'] - test['distance']) / test['num_gt']\n",
    "    except ZeroDivisionError:\n",
    "        result['reduction'] = np.NaN\n",
    "    return result\n",
    "\n",
    "def calculate_utility_other(test):\n",
    "    result = {}\n",
    "    try:\n",
    "        result['reduction'] = (test['num_gt'] - test['distance']) / test['num_gt']\n",
    "    except ZeroDivisionError:\n",
    "        result['reduction'] = np.NaN\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1/6 processing steps...\n",
      "Done 2/6 processing steps...\n",
      "Done 3/6 processing steps...\n",
      "Done 4/6 processing steps...\n",
      "Done 5/6 processing steps...\n",
      "Done 6/6 processing steps! :) Now writing intermediate results to file...\n",
      "All done! :D\n"
     ]
    }
   ],
   "source": [
    "app_category = 'news'\n",
    "\n",
    "perfect_csv = []\n",
    "for path in glob.glob(\"input/perfect/mapping_results_\" + app_category + \"/*.csv\"):\n",
    "    csv = read_csv(path)\n",
    "    apps = os.path.splitext(os.path.basename(path))[0].split(\"_\")\n",
    "    csv['source'] = csv.apply(lambda x: apps[0], axis=1)\n",
    "    csv['target'] = csv.apply(lambda x: apps[1], axis=1)\n",
    "    csv['gui_mapper'] = csv.apply(lambda x: \"perfect\", axis=1)\n",
    "    perfect_csv.append(csv)\n",
    "combined_csv = pd.concat(perfect_csv)\n",
    "\n",
    "appflow_csv = []\n",
    "for path in glob.glob(\"input/appflow/mapping_results_\" + app_category + \"/*.csv\"):\n",
    "    csv = read_csv(path)\n",
    "    apps = os.path.splitext(os.path.basename(path))[0].split(\"_\")\n",
    "    csv['source'] = csv.apply(lambda x: apps[0], axis=1)\n",
    "    csv['target'] = csv.apply(lambda x: apps[1], axis=1)\n",
    "    csv['gui_mapper'] = csv.apply(lambda x: \"appflow\", axis=1)\n",
    "    appflow_csv.append(csv)\n",
    "combined_csv = combined_csv.append(appflow_csv)\n",
    "\n",
    "naive_csv = []\n",
    "filename = \"input/naive/mapping_results_\" + app_category + \"/*.csv\"\n",
    "for path in glob.glob(filename):\n",
    "    csv = read_csv(path)\n",
    "    apps = os.path.splitext(os.path.basename(path))[0].split(\"_\")\n",
    "    csv['source'] = csv.apply(lambda x: apps[0], axis=1)\n",
    "    csv['target'] = csv.apply(lambda x: apps[1], axis=1)\n",
    "    csv['gui_mapper'] = csv.apply(lambda x: \"naive\", axis=1)\n",
    "    naive_csv.append(csv)\n",
    "combined_csv = combined_csv.append(naive_csv)\n",
    "\n",
    "combined_csv['event_array'] = combined_csv['event_array'].apply(json.loads)\n",
    "\n",
    "ground_truth_tests = [read_csv(path, header=0) for path in glob.glob(\"input/extracted_tests/\" + app_category + \"/*.csv\")]\n",
    "ground_truth_tests = pd.concat(ground_truth_tests)\n",
    "ground_truth_tests['event_array'] = ground_truth_tests['event_array'].apply(json.loads)\n",
    "\n",
    "combined_csv = combined_csv.apply(evaluate_accuracy, axis=1)\n",
    "print('Done 1/6 processing steps...')\n",
    "combined_csv = pd.concat([combined_csv, combined_csv['event_array'].apply(list_cases).apply(pd.Series)], axis=1)\n",
    "print('Done 2/6 processing steps...')\n",
    "combined_csv = pd.concat([combined_csv, combined_csv.apply(count_cases, axis=1).apply(pd.Series)], axis=1)\n",
    "print('Done 3/6 processing steps...')\n",
    "combined_csv = pd.concat([combined_csv, combined_csv.apply(calc_precision_recall_accuracy, axis=1).apply(pd.Series)], axis=1)\n",
    "print('Done 4/6 processing steps...')\n",
    "combined_csv = pd.concat([combined_csv, combined_csv.apply(append_src_gt_events, axis=1).apply(pd.Series)], axis=1)\n",
    "print('Done 5/6 processing steps...')\n",
    "combined_csv['event_array'] = combined_csv['event_array'].apply(json.dumps)\n",
    "combined_csv = pd.concat([combined_csv, combined_csv.apply(levenshtein, axis=1).apply(pd.Series)], axis=1)\n",
    "print('Done 6/6 processing steps! :) Now writing intermediate results to framework_results_news.csv...')\n",
    "combined_csv.to_csv(\"output/framework_results_\" + app_category + \".csv\", index=False)\n",
    "print('All done! :D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 1/6 processing steps...\n",
      "Done 2/6 processing steps...\n",
      "Done 3/6 processing steps...\n",
      "Done 4/6 processing steps...\n",
      "Done 5/6 processing steps...\n",
      "Done 6/6 processing steps! :) Now writing intermediate results to file...\n",
      "All done! :D\n"
     ]
    }
   ],
   "source": [
    "app_name_mapping = {}\n",
    "app_name_mapping['5miles'] = 'FiveMiles'\n",
    "app_name_mapping['6pm'] = 'SixPM'\n",
    "app_name_mapping['aliexpress'] = 'AliExpress'\n",
    "app_name_mapping['ebay'] = 'Ebay'\n",
    "app_name_mapping['etsy'] = 'Etsy'\n",
    "app_name_mapping['geek'] = 'Geek'\n",
    "app_name_mapping['googleshopping'] = 'GoogleExpress'\n",
    "app_name_mapping['groupon'] = 'Groupon'\n",
    "app_name_mapping['home'] = 'Home'\n",
    "app_name_mapping['wish'] = 'Wish'\n",
    "\n",
    "app_category = 'shopping'\n",
    "\n",
    "perfect_csv = []\n",
    "for path in glob.glob(\"input/perfect/mapping_results_\" + app_category + \"/*.csv\"):\n",
    "    csv = read_csv(path)\n",
    "    apps = os.path.splitext(os.path.basename(path))[0].split(\"_\")\n",
    "    csv['source'] = csv.apply(lambda x: apps[0], axis=1)\n",
    "    csv['target'] = csv.apply(lambda x: apps[1], axis=1)\n",
    "    csv['gui_mapper'] = csv.apply(lambda x: \"perfect\", axis=1)\n",
    "    perfect_csv.append(csv)\n",
    "combined_csv = pd.concat(perfect_csv)\n",
    "\n",
    "appflow_csv = []\n",
    "for path in glob.glob(\"input/appflow/mapping_results_\" + app_category + \"/*.csv\"):\n",
    "    csv = read_csv(path)\n",
    "    apps = os.path.splitext(os.path.basename(path))[0].split(\"_\")\n",
    "    csv['source'] = csv.apply(lambda x: apps[0], axis=1)\n",
    "    csv['target'] = csv.apply(lambda x: apps[1], axis=1)\n",
    "    csv['gui_mapper'] = csv.apply(lambda x: \"appflow\", axis=1)\n",
    "    appflow_csv.append(csv)\n",
    "combined_csv = combined_csv.append(appflow_csv)\n",
    "\n",
    "craftdroid_csv = []\n",
    "for path in glob.glob(\"input/craftdroid/mapping_results/*.csv\"):\n",
    "    csv = read_csv(path)\n",
    "    apps = os.path.splitext(os.path.basename(path))[0].split(\"_\")\n",
    "    csv['source'] = csv.apply(lambda x: apps[0], axis=1)\n",
    "    csv['target'] = csv.apply(lambda x: apps[1], axis=1)\n",
    "    csv['gui_mapper'] = csv.apply(lambda x: \"craftdroid\", axis=1)\n",
    "    craftdroid_csv.append(csv)\n",
    "# combined_csv is based on the mapping results, i.e., transferred tests\n",
    "combined_csv = combined_csv.append(craftdroid_csv)\n",
    "\n",
    "naive_csv = []\n",
    "filename = \"input/naive/mapping_results_\" + app_category + \"/*.csv\"\n",
    "for path in glob.glob(filename):\n",
    "    csv = read_csv(path)\n",
    "    apps = os.path.splitext(os.path.basename(path))[0].split(\"_\")\n",
    "    csv['source'] = csv.apply(lambda x: apps[0], axis=1)\n",
    "    csv['target'] = csv.apply(lambda x: apps[1], axis=1)\n",
    "    csv['gui_mapper'] = csv.apply(lambda x: \"naive\", axis=1)\n",
    "    naive_csv.append(csv)\n",
    "combined_csv = combined_csv.append(naive_csv)\n",
    "\n",
    "combined_csv['event_array'] = combined_csv['event_array'].apply(json.loads)\n",
    "\n",
    "ground_truth_tests = [read_csv(path, header=0) for path in glob.glob(\"input/extracted_tests/\" + app_category + \"/*.csv\")]\n",
    "ground_truth_tests = pd.concat(ground_truth_tests)\n",
    "ground_truth_tests['event_array'] = ground_truth_tests['event_array'].apply(json.loads)\n",
    "\n",
    "combined_csv = combined_csv.apply(evaluate_accuracy, axis=1)\n",
    "print('Done 1/6 processing steps...')\n",
    "combined_csv = pd.concat([combined_csv, combined_csv['event_array'].apply(list_cases).apply(pd.Series)], axis=1)\n",
    "print('Done 2/6 processing steps...')\n",
    "combined_csv = pd.concat([combined_csv, combined_csv.apply(count_cases, axis=1).apply(pd.Series)], axis=1)\n",
    "print('Done 3/6 processing steps...')\n",
    "combined_csv = pd.concat([combined_csv, combined_csv.apply(calc_precision_recall_accuracy, axis=1).apply(pd.Series)], axis=1)\n",
    "print('Done 4/6 processing steps...')\n",
    "combined_csv = pd.concat([combined_csv, combined_csv.apply(append_src_gt_events, axis=1).apply(pd.Series)], axis=1)\n",
    "print('Done 5/6 processing steps...')\n",
    "combined_csv['event_array'] = combined_csv['event_array'].apply(json.dumps)\n",
    "combined_csv = pd.concat([combined_csv, combined_csv.apply(levenshtein, axis=1).apply(pd.Series)], axis=1)\n",
    "print('Done 6/6 processing steps! :) Now writing intermediate results to framework_results_shopping.csv...')\n",
    "combined_csv.to_csv(\"output/framework_results_\" + app_category + \".csv\", index=False)\n",
    "print('All done! :D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Done 1/5 processing steps...\n",
      "Done 2/5 processing steps...\n",
      "Done 3/5 processing steps...\n",
      "Done 4/5 processing steps... Now writing news_final.csv to output folder\n",
      "Done 5/5 processing steps! :) Now writing shopping_final.csv to output folder\n",
      "All done! :D\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# add 'reduction' to final results to ATM mapper\n",
    "atm_csv = read_csv(\"input/ATM/atm_shopping.csv\")\n",
    "atm_csv = pd.concat([atm_csv, atm_csv.apply(count_event_num_atm, axis=1).apply(pd.Series)], axis=1)\n",
    "atm_csv = pd.concat([atm_csv, atm_csv.apply(calculate_utility_atm, axis=1).apply(pd.Series)], axis=1)\n",
    "atm_csv.to_csv(\"output/atm_shopping.csv\", index=False)\n",
    "atm_csv = read_csv(\"input/ATM/atm_news.csv\")\n",
    "atm_csv = pd.concat([atm_csv, atm_csv.apply(count_event_num_atm, axis=1).apply(pd.Series)], axis=1)\n",
    "atm_csv = pd.concat([atm_csv, atm_csv.apply(calculate_utility_atm, axis=1).apply(pd.Series)], axis=1)\n",
    "atm_csv.to_csv(\"output/atm_news.csv\", index=False)\n",
    "print('Done 1/5 processing steps...')\n",
    "\n",
    "# add 'reduction' to final results to GTM mapper\n",
    "gtm_csv = read_csv(\"input/GTM/gtm_shopping.csv\")\n",
    "gtm_csv = pd.concat([gtm_csv, gtm_csv.apply(count_event_num_atm, axis=1).apply(pd.Series)], axis=1)\n",
    "gtm_csv = pd.concat([gtm_csv, gtm_csv.apply(calculate_utility_atm, axis=1).apply(pd.Series)], axis=1)\n",
    "gtm_csv.to_csv(\"output/gtm_shopping.csv\", index=False)\n",
    "gtm_csv = read_csv(\"input/GTM/gtm_news.csv\")\n",
    "gtm_csv = pd.concat([gtm_csv, gtm_csv.apply(count_event_num_atm, axis=1).apply(pd.Series)], axis=1)\n",
    "gtm_csv = pd.concat([gtm_csv, gtm_csv.apply(calculate_utility_atm, axis=1).apply(pd.Series)], axis=1)\n",
    "gtm_csv.to_csv(\"output/gtm_news.csv\", index=False)\n",
    "print('Done 2/5 processing steps...')\n",
    "\n",
    "# add 'reduction' to final results of other mappers\n",
    "other_csv = read_csv(\"output/framework_results_news.csv\")\n",
    "other_csv = other_csv.rename(columns={'event_array': 'transferred'})\n",
    "other_csv = pd.concat([other_csv, other_csv.apply(count_event_num_other, axis=1).apply(pd.Series)], axis=1)\n",
    "other_csv = pd.concat([other_csv, other_csv.apply(calculate_utility_other, axis=1).apply(pd.Series)], axis=1)\n",
    "other_csv.to_csv(\"output/other_news.csv\", index=False)\n",
    "other_csv = read_csv(\"output/framework_results_shopping.csv\")\n",
    "other_csv = other_csv.rename(columns={'event_array': 'transferred'})\n",
    "other_csv = pd.concat([other_csv, other_csv.apply(count_event_num_other, axis=1).apply(pd.Series)], axis=1)\n",
    "other_csv = pd.concat([other_csv, other_csv.apply(calculate_utility_other, axis=1).apply(pd.Series)], axis=1)\n",
    "other_csv.to_csv(\"output/other_shopping.csv\", index=False)\n",
    "print('Done 3/5 processing steps...')\n",
    "\n",
    "# merge all results together into news_final.csv\n",
    "atm_csv = read_csv(\"output/atm_news.csv\")\n",
    "gtm_csv = read_csv(\"output/gtm_news.csv\")\n",
    "other_csv = read_csv(\"output/other_news.csv\") \n",
    "merged_csv = atm_csv.append(gtm_csv)\n",
    "merged_csv = merged_csv.append(other_csv)\n",
    "print('Done 4/5 processing steps... Now writing news_final.csv to output folder')\n",
    "merged_csv.to_csv(\"output/news_final.csv\", index=False)\n",
    "\n",
    "# merge all results together into shopping_final.csv\n",
    "atm_csv = read_csv(\"output/atm_shopping.csv\")\n",
    "gtm_csv = read_csv(\"output/gtm_shopping.csv\")\n",
    "other_csv = read_csv(\"output/other_shopping.csv\") \n",
    "merged_csv = atm_csv.append(gtm_csv)\n",
    "merged_csv = merged_csv.append(other_csv)\n",
    "print('Done 5/5 processing steps! :) Now writing shopping_final.csv to output folder')\n",
    "merged_csv.to_csv(\"output/shopping_final.csv\", index=False)\n",
    "print('All done! :D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}